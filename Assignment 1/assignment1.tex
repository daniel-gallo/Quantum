\documentclass{article}

\usepackage{tcolorbox}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}
\usepackage{mathtools}

\def\H{\mathcal{H}}
\def\B{\mathcal{B}}
\def\C{\mathbb{C}}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\abs}[1]{\lvert}{\rvert}{#1}

\setlength\parindent{0pt}

\title{Assignment 1}
\author{Daniel Gallo}

\begin{document}
    \maketitle

    \begin{tcolorbox}[title=Exercise 1]
        Consider a Hilbert space $\H$ with basis $\{\ket{1}, \ket{2}\}$ such that $\braket{1|1} = 5$, $\braket{1|2} = -3i$ and $\braket{2|2} = 2$.
        \begin{enumerate}
            \item Let $\ket{v} = (i\sqrt{2}, 3)$ and $\ket{w} = (2\sqrt{2}, 1 + 3i)$. Compute $\norm{\ket{v}}$, $\norm{\ket{w}}$ and $\braket{v | w}$.
            \item Use Gram-Schmidt to obtain an orthonormal basis $\B$.
            \item Find the column vector representation of $(\alpha, \beta)$ with respect to $\B$.
            \item Find row vector representations for $\bra{v}$ and $\bra{w}$ with respect to to the dual basis $\B^*$
            \item Let $\hat{A}$ be a linear operator on $\H$ such that
            \begin{align*}
                \hat{A} \ket{1} &= \ket{1} \\
                \hat{A} \ket{2} &= -\frac{6i}{5} \ket{1} - \ket{2}
            \end{align*}
            Find the matrix representation of $\hat{A}$ with respecto to $\B$.
        \end{enumerate}
    \end{tcolorbox}
    \begin{enumerate}
        \item 
        \begin{equation*}
            \braket{v | v} =
            \begin{bmatrix}
                -i\sqrt{2} & 3
            \end{bmatrix}
            \begin{bmatrix}
                5 & -3i \\
                3i & 2
            \end{bmatrix}
            \begin{bmatrix}
                i\sqrt{2} \\
                3
            \end{bmatrix}
            = 28 - 18\sqrt{2}
        \end{equation*}
        We can conclude that $\norm{\ket{v}} \approx 1.595$
        \begin{equation*}
            \braket{w | w} = 
            \begin{bmatrix}
                2\sqrt{2} & 1 - 3i
            \end{bmatrix}
            \begin{bmatrix}
                5 & -3i \\
                3i & 2
            \end{bmatrix}
            \begin{bmatrix}
                2\sqrt{2} \\
                1 + 3i
            \end{bmatrix}
            = 60 + 36\sqrt{2}
        \end{equation*}
        Thus, $\norm{\ket{w}} \approx 10.531$
        \begin{equation*}
            \braket{v | w} = 
            \begin{bmatrix}
                -i\sqrt{2} & 3
            \end{bmatrix}
            \begin{bmatrix}
                5 & -3i \\
                3i & 2
            \end{bmatrix}
            \begin{bmatrix}
                2\sqrt{2} \\
                1 + 3i
            \end{bmatrix}
            = 6 + 38i
        \end{equation*}

        \item We can set
        \begin{align*}
            \ket{v_1} &= \ket{1} \\
            \ket{v_2} &= \ket{2} - \frac{\braket{1|2}}{\braket{1|1}} \ket{1} = \frac{3i}{5} \ket{1} + \ket{2}
        \end{align*}
        If we normalize them, we get,
        \begin{align*}
            \ket{e_1} &= \frac{1}{\sqrt{5}} \ket{1} \\
            \ket{e_2} &= \frac{3}{\sqrt{5}}i \ket{1} + \sqrt{5}\ket{2}
        \end{align*}

        \item We want to find $x, y \in \C$ such that $x\ket{e_1} + y\ket{e_2} = \alpha\ket{1} + \beta\ket{2}$.
        \begin{align*}
            \begin{bmatrix}
                x \\
                y
            \end{bmatrix}
            &=
            \begin{bmatrix}
                \frac{1}{\sqrt{5}}  & \frac{3i}{\sqrt{5}} \\
                0 & \sqrt{5}
            \end{bmatrix}^{-1}
            \begin{bmatrix}
                \alpha \\
                \beta
            \end{bmatrix}
            = \\
            &=
            \begin{bmatrix}
                \sqrt{5} & \frac{-3i}{\sqrt{5}} \\
                0 & \frac{1}{\sqrt{5}}
            \end{bmatrix}
            \begin{bmatrix}
                \alpha \\
                \beta
            \end{bmatrix} = 
            \begin{bmatrix}
                \sqrt{5} \alpha -\frac{3i\beta}{\sqrt{5}} \\
                \frac{\beta}{\sqrt{5}}
            \end{bmatrix}
        \end{align*}

        \item Let's express $\ket{v}$ and $\ket{w}$ in $\B$.
        \begin{equation*}
            \ket{v} = 
            \begin{bmatrix}
                \sqrt{5} & \frac{-3i}{\sqrt{5}} \\
                0 & \frac{1}{\sqrt{5}}
            \end{bmatrix}
            \begin{bmatrix}
                \sqrt{2}i \\
                3
            \end{bmatrix}
            =
            \begin{bmatrix}
                \frac{i}{\sqrt{5}} (-9 + 5\sqrt{2})\\
                \frac{3}{\sqrt{5}}
            \end{bmatrix}
        \end{equation*}
        \begin{equation*}
            \ket{w} = 
            \begin{bmatrix}
                \sqrt{5} & \frac{-3i}{\sqrt{5}} \\
                0 & \frac{1}{\sqrt{5}}
            \end{bmatrix}
            \begin{bmatrix}
                2\sqrt{2} \\
                1 + 3i
            \end{bmatrix}
            =
            \begin{bmatrix}
                \frac{1}{\sqrt{5}} (9 + 10\sqrt{2} - 3i)\\
                \frac{1}{\sqrt{5}} (1 + 3i)
            \end{bmatrix}
        \end{equation*}
        Since $\B$ is an orthonormal basis,
        \begin{align*}
            \bra{v} &=
            \begin{bmatrix}
                \frac{i}{\sqrt{5}} (9 - 5\sqrt{2}) & \frac{3}{\sqrt{5}}
            \end{bmatrix} \\
            \bra{w} &=
            \begin{bmatrix}
                \frac{1}{\sqrt{5}} (9 + 10\sqrt{2} + 3i) & \frac{1}{\sqrt{5}} (1 - 3i)
            \end{bmatrix}
        \end{align*}

        \item
        Let $M$ be the chage of basis matrix (from $\B$ to the original one).
        \begin{equation*}
            M = 
            \begin{bmatrix}
                \frac{1}{\sqrt{5}}  & \frac{3i}{\sqrt{5}} \\
                0 & \sqrt{5}
            \end{bmatrix}
        \end{equation*}
        Then,
        \begin{align*}
            A &= M^{-1}
            \begin{bmatrix}
                1 & -\frac{6i}{5} \\
                0 & -1
            \end{bmatrix}
            M \\
            &=
            \begin{bmatrix}
                1 & 0 \\
                0 & -1
            \end{bmatrix}
        \end{align*}
    \end{enumerate}

    \begin{tcolorbox}[title=Exercise 2]
        Let $\H$ be a finite-dimensional Hilbert space.
        \begin{enumerate}
            \item Recall the definition of the Hilbert-Schmidt inner product on $\End(\H)$. Explicitly show that it defines an inner product. 
            \item Find an orthonormal basis on $\End(\H)$ with respect to this inner product. Express your solution in the outer product notation and state the dimension of $\End(\H)$.
            \item Find an orthonormal basis of Hermitian matrices for $\End(\H)$ with respect to this inner product.
        \end{enumerate}
    \end{tcolorbox}

    \begin{enumerate}
        \item The Hilbert-Schmidt inner product is defined as follows
        \begin{align*}
            \langle\cdot,\cdot\rangle \colon \End(\H) &\to \End(\H) \\
            A, B &\mapsto \tr(A^\dagger B)
        \end{align*}
        Let's verify that it is indeed an inner product.
        \begin{enumerate}
            \item \textbf{Linearity} in the second component
            \begin{align*}
                \langle A, \beta_1 B_1 + \beta_2 B_2\rangle &= \tr(A^\dagger (\beta_1 B_1 + \beta_2 B_2)) \\
                &= \tr(\beta_1 A^\dagger B_1 + \beta_2 A^\dagger B_2) \\
                &= \beta_1\tr(A^\dagger B_1) + \beta_2\tr(A^\dagger B_2) \\
                &= \beta_1\langle A, B_1\rangle + \beta_2\langle A, B_2\rangle
            \end{align*}
            \item \textbf{Skew-symmetry}
            \begin{align*}
                \langle A, B\rangle &= \tr(A^\dagger B) = \tr((A^\dagger B)^\dagger)^*\\
                &= \tr(B^\dagger A)^* = \langle B, A\rangle^*
            \end{align*}
            \item \textbf{Positive-definiteness}
            \begin{equation*}
                \langle A, A\rangle = \tr(A^\dagger A) = \sum_{i,j} \abs{A_{ij}}^2 \geq 0
            \end{equation*}
            Furthermore, $\sum_{i,j} \abs{A_{ij}}^2 = 0$ if and only if $A = 0$.
        \end{enumerate}

        \item 
        \item Recall that the Hermitian matrices satisfy $A^\dagger = A$, so $A_{ij} = A_{ji}^*$, which means that diagonal elements must be purely real. If $\dim{\H} = n$ we will have a basis of $n^2$ elements. If we index those elements in an array-like form, we can create the basis elements as follows.
        \begin{itemize}
            \item If $i < j$, a matrix with $M_{ij} = \frac{1}{\sqrt{2}}$ and $M_{ji} = \frac{1}{\sqrt{2}}$ and $0$ elsewhere.
            \item If $i = j$, a matrix with a one in the $i$th element of the diagonal, and $0$ elsewhere.
            \item If $i > j$, a matrix with $M_{ij} = \frac{i}{\sqrt{2}}$ and $M_{ji} = -\frac{i}{\sqrt{2}}$ and $0$ elsewhere.
        \end{itemize}
        All basis elements have length 1, which can be easily seen by noting that $\langle M, M\rangle = \tr(M^\dagger M) = \sum_{i,j} \abs{M_{ij}}^2 = 1$. Now we have to prove that the elements are indeed orthogonal.
    \end{enumerate}
    
    \begin{tcolorbox}[title=Exercise 3]
        Consider the Pauli matrices $\{\sigma_i\}_{i=0}^3$. Let $\epsilon_{jkl}$ denote the Levi-Civita symbol.
        \begin{enumerate}
            \item Find the eigenvectors, eigenvalues and diagonal matrix representations for each of the Pauli matrices.
            \item Show that the Pauli matrices constitute a basis for the complex vector space $M_2(\C)$.
            \item Show that for $k, j \in \{1,2,3\}$ we have
            \begin{equation*}
                \left[\sigma_j, \sigma_k\right] = 2i\sum_{l=1}^3 \epsilon_{jkl} \sigma_l
            \end{equation*}
            \begin{equation*}
                \sigma_j \sigma_k = \delta_{jk}\sigma_0 + i\sum_{l=1}^3 \epsilon_{jkl} \sigma{l} 
            \end{equation*}
        \end{enumerate}
    \end{tcolorbox}

    \begin{enumerate}
        \item Recall that the eigenvalues are obtained by solving $\det(\sigma_j - \lambda I) = 0$. To obtain the eigenvectors (the columns of the change of basis matrix) we have to find $\ker(\sigma_j - \lambda I)$. Since the Pauli matrices are Hermitian, we can diagonalize them. In fact, by the \textit{Spectral Theorem} we can find an orthonormal basis of the 
        \begin{enumerate}
            \item
            \begin{equation*}
                \sigma_0 =
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 1
                \end{bmatrix}
                =
                \underbrace{
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 1
                \end{bmatrix}
                }_P
                \underbrace{
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 1
                \end{bmatrix}
                }_D
                \underbrace{
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 1
                \end{bmatrix}^{-1}
                }_{P^{-1}}
            \end{equation*}
            \item
            \begin{equation*}
                \sigma_1 =
                \begin{bmatrix}
                    0 & 1 \\
                    1 & 0
                \end{bmatrix}
                =
                \underbrace{
                \begin{bmatrix}
                    1 & 1 \\
                    1 & -1
                \end{bmatrix}
                }_P
                \underbrace{
                \begin{bmatrix}
                    1 & 0 \\
                    0 & -1
                \end{bmatrix}
                }_D
                \underbrace{
                \begin{bmatrix}
                    1 & 1 \\
                    1 & -1
                \end{bmatrix}^{-1}
                }_{P^{-1}}
            \end{equation*}
            \item
            \begin{equation*}
                \sigma_2 =
                \begin{bmatrix}
                    0 & -i \\
                    i & 0
                \end{bmatrix}
                =
                \underbrace{
                \begin{bmatrix}
                    1 & 1 \\
                    i & -i
                \end{bmatrix}
                }_P
                \underbrace{
                \begin{bmatrix}
                    1 & 0 \\
                    0 & -1
                \end{bmatrix}
                }_D
                \underbrace{
                \begin{bmatrix}
                    1 & 1 \\
                    i & -i
                \end{bmatrix}^{-1}
                }_{P^{-1}}
            \end{equation*}
            \item
            \begin{equation*}
                \sigma_3 =
                \begin{bmatrix}
                    1 & 0 \\
                    0 & -1
                \end{bmatrix}
                =
                \underbrace{
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 1
                \end{bmatrix}
                }_P
                \underbrace{
                \begin{bmatrix}
                    1 & 0 \\
                    0 & -1
                \end{bmatrix}
                }_D
                \underbrace{
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 1
                \end{bmatrix}^{-1}
                }_{P^{-1}}
            \end{equation*}
        \end{enumerate}
        \item $M_2(\C)$ has dimension 4, and $\{\sigma_i\}_{i=0}^3$ has 4 elements, so we just have to prove that all of them are mutually orthogonal. Recall that the Hilbert-Schmidt's inner product is defined as
        \begin{align*}
            \langle\cdot,\cdot\rangle \colon M_2(\C) &\to M_2(\C) \\
            A, B &\mapsto \tr(A^\dagger B) = \sum_{ij} A_{ij}^* B_{ij}
        \end{align*}
        Checking the six possible combinations $\langle \sigma_i, \sigma_j \rangle$ for $0 \leq i < j \leq 3$, we see that in fact they are mutually orthogonal.
        \item TODO
    \end{enumerate}
    \begin{tcolorbox}[title=Exercise 4]
        Consider the commutator
        \begin{equation*}
            \left[\underbrace{x^a \frac{d^2}{dx^2}}_A, \underbrace{x^b \frac{d}{dx}}_B\right]
        \end{equation*}
        and find the values of $a$ and $b$ for which it vanishes. Show that any eigenstate of $x\frac{d}{dx}$ with eigenvalue $\lambda$ is also an eigenstate of $x^2\frac{d^2}{dx^2}$ and find the corresponding eigenvalue.
    \end{tcolorbox}
    \textit{At first I thought that functions applied by these operators were polynomial, but then I worked it out for more general smooth functions (with at least three derivatives)} \par
    \subsubsection*{For polynomial}
    The commutator vanishes if and only if $AB = BA$.
    \begin{align*}
        AB &= \sum_{i=1}^\infty n (b + n - 1) (b + n - 2) a_n x^{a + b + n - 3} \\
        BA &= \sum_{i=1}^\infty n (n + 1) (a + n - 1) a_{n + 1} x^{a + b + n - 2}
    \end{align*}
    Note that we need the terms $a_j x^{a + b + j - 3}$ to align up, since the sum has to hold for every $n$ and $x$. Also note that $AB$ has an extra term including $a_1 x^{a + b - 2}$ that $BA$ does not. This term is
    \begin{equation*}
        b (b - 1) a_1 x^{a + b - 2}
    \end{equation*}
    To get rid of it, we need to set $b = 0$ or $b = 1$. To find out the value of $a$, let us shift the $AB$ sum so it doesn't include its first term.
    \begin{align*}
        AB &= \sum_{i=1}^\infty (n + 1) (b + n) (b + n - 1) a_{n + 1} x^{a + b + n - 2} \\
        BA &= \sum_{i=1}^\infty n (n + 1) (a + n - 1) a_{n + 1} x^{a + b + n - 2}
    \end{align*}
    Setting $b = 0$, we get
    \begin{equation*}
        n (n - 1) = n (a + n - 1) \implies a = 0
    \end{equation*}
    Setting $b = 1$, we get
    \begin{equation*}
        n (n + 1) = n (a + n - 1) \implies a = 2
    \end{equation*}
    Thus, if $(a, b) = (0, 0)$ or $(a, b) = (2, 1)$ the commutator vanishes. \par
    
    \subsubsection*{For smooth functions}
    \begin{align*}
        AB(f) &= b (b - 1) x^{a + b - 2} f'(x) + 2 b x^{a + b - 1} f''(x) + x^{a + b} f'''(x) \\
        BA(f) &= a x^{a + b - 1} f''(x) + x^{a + b} f'''(x)
    \end{align*}
    We can see that we need $b = 0$ or $b = 1$ and $2b = a$, so $(a, b) = (0, 0)$ or $(a, b) = (2, 1)$, as before. \par
    If $x \frac{d}{dx}$ has eigenvalue $\lambda$, that means that 
    \begin{equation}
        \label{eq:ode}
        x f'(x) = \lambda f(x)
    \end{equation}
    If we differentiate \eqref{eq:ode}, we get
    \begin{equation*}
        f'(x) + xf''(x) = \lambda f'(x)
    \end{equation*}
    Multiplying by $x$ and rearranging,
    \begin{align*}
        x^2 f''(x) &= (\lambda - 1) x f'(x) & \text{By \eqref{eq:ode}}\\
        &= (\lambda - 1) \lambda f(x)
    \end{align*}
    We can conclude that any eigenstate of $x \frac{d}{dx}$ with eigenvalue $\lambda$ is an eigenstate of $x^2 \frac{d^2}{dx^2}$ with eigenvalue $(\lambda - 1) \lambda$.
    \begin{tcolorbox}[title=Exercise 5]
        Let $\H$ be a finite-dimensional Hilbert space.
        \begin{enumerate}
            \item Show that for any linear operators $\hat{A}$ and $\hat{B}$, and for any $\ket{v}, \ket{w} \in \H$ we have
            \begin{equation*}
                \left(\braket{v|\hat{A}|w}B\right)^\dagger = \braket{w|\hat{A}|v}B^\dagger
            \end{equation*}
            \item Show (without using the spectral theorem) that the eigenspaces of a unitary operator on $\H$ are mutually orthogonal
            \item Show that a positive operator on $\H$ has a unique positive square root. You may use the spectral theorem and the fact that a positive operator is necessarily Hermitian.
        \end{enumerate}
    \end{tcolorbox}
    \begin{enumerate}
        \item \begin{align*}
            \left(\braket{v|\hat{A}|w}B\right)^\dagger &= \braket{v|\hat{A}|w}^* B^\dagger & \text{since $\braket{v|\hat{A}|w}$ is a number} \\
            &= \braket{v|\hat{A}|w}^\dagger B^\dagger & \text{$\dagger$ and $*$ are the same for numbers} \\
            &= \braket{w|\hat{A}^\dagger|v} B^\dagger & \text{by the reverse property of $\dagger$} \\
        \end{align*}
        \item First we will show that the eigenvalues of a unitary operator lie in the unit circle. Let $\ket{v}$ be an eigenvector of the eigenvalue $\lambda$, that is, $\hat{A}\ket{v} = \lambda \ket{v}$. Then,
        \begin{align*}
            \hat{A}\ket{v} \cdot \hat{A}\ket{v} &= \braket{v | \hat{A}^\dagger \hat{A} | v} = \braket{v | v} \\
            &= \lambda \ket{v} \cdot \lambda \ket{v} = \abs{\lambda}^2 \braket{v | v}
        \end{align*}
        Since $\ket{v} \neq 0$, $\abs{\lambda}^2$ = 1. To prove that the eigenspaces are orthogonal, take $\ket{v} \in E_{\lambda_1}$ and $\ket{w} \in E_{\lambda_2}$. Because of what we have just proved, we can assume
        \begin{align*}
            \hat{A} \ket{v} &= e^{i\theta_1} \ket{v} \\
            \hat{A} \ket{w} &= e^{i\theta_2} \ket{w}
        \end{align*}
        If we compute $\hat{A} \ket{v} \cdot \hat{A} \ket{w}$ we get
        \begin{equation*}
            \braket{v | w} = e^{i(\theta_2 - \theta_1)} \braket{v | w}
        \end{equation*}
        Since $e^{i\theta_1}$ was supposed to be different than $e^{i\theta_2}$, we can conclude that $\braket{v | w} = 0$, that is $\ket{v}$ and $\ket{w}$ are orthogonal.
        \item First we will prove that there exists at least one positive square root. Let $A$ be a matrix representation of the positive operator $\hat{A}$. Since $\hat{A}$ is positive, $\hat{A}$ is Hermitian, and by the \textit{Spectral Theorem} we can express $A$ as follows
        \begin{equation*}
            A = P D P^{-1} = P
            \begin{bmatrix}
                \lambda_1&&& \\
                &\lambda_2&& \\
                &&\ddots \\
                &&&\lambda_n
            \end{bmatrix}
            P^{-1}
        \end{equation*}
        Since we know that $\hat{A}$ is positive, $\lambda_i \geq 0$ for all $i$, and we can define $\sqrt{A}$ as
        \begin{equation*}
            \sqrt{A} = P \sqrt{D} P^{-1} = P
            \begin{bmatrix}
                \sqrt{\lambda_1}&&& \\
                &\sqrt{\lambda_2}&& \\
                &&\ddots \\
                &&&\sqrt{\lambda_n}
            \end{bmatrix}
            P^{-1}
        \end{equation*}
        As one can easily check, $\sqrt{A}\sqrt{A} = P \sqrt{D} P^{-1} P \sqrt{D} P^{-1} = P D P^{-1} = A$. Note that $\sqrt{A}$ is still positive because $\sqrt{\lambda_i} \geq 0$ for all $i$. TODO (uniqueness)
    \end{enumerate}
\end{document}